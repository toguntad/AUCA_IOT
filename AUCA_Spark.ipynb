{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd2cb12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33111180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                    \t\t      .master(\"local[*]\") \\\n",
    "                   \t\t      .appName(\"Word Count\") \\\n",
    "                    \t\t      .config(\"spark.some.config.option\", \"SparkSessionExample\") \\\n",
    "                    \t\t      .getOrCreate()\n",
    "# Example action: Creates a DataFrame with numbers from 0 to 9 and gathers them into the driver’s \n",
    "# memory and creates a list.\n",
    "print(spark.range(10).collect())\n",
    "\n",
    "# stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea55e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get a Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Example Application\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Access SparkContext from SparkSession\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2875bbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now you can use `sc` to perform RDD operations, for example:\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "print(rdd.collect())\n",
    "\n",
    "# Always stop the Spark session when you're done\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee9763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count: output to file\n",
    "# Assuming you're using local files, but adjust the path as necessary\n",
    "file = sc.textFile(\"file:///path/to/your/1112.txt\")\n",
    "# Split each line into words using flatMap\n",
    "words = file.flatMap(lambda line: line.split(\" \"))\n",
    "# Map words to (word, 1) pairs\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "# Reduce by key to count each word\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "# Save the result to a local file system or HDFS\n",
    "counts.saveAsTextFile(\"file:///path/to/output\")  # Change this path if using HDFS or another storage location\n",
    "\n",
    "# Always stop the Spark session when you're done\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count: output to terminal\n",
    "file = sc.textFile(\"file:///path/to/your/1112.txt\")\n",
    "words = file.flatMap(lambda line: line.split(\" \"))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect results and print them\n",
    "results = counts.collect()\n",
    "for (word, count) in results:\n",
    "    print(word, count)\n",
    "\n",
    "# Always stop the Spark session when you're done\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a5305",
   "metadata": {},
   "source": [
    "## SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e244b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Word Count from JSON\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from a JSON file\n",
    "df = spark.read.json(\"input.json\")\n",
    "\n",
    "# Assuming the JSON contains a field 'text' that contains text data\n",
    "# The explode and split functions are used to transform the text into individual words. \n",
    "# The split function splits the text into a list of words, and explode creates a new row for each word.\n",
    "words = df.select(explode(split(df.text, \"\\\\s+\")).alias(\"word\"))\n",
    "\n",
    "# The words are then grouped by the word itself, and count() is applied to compute the\n",
    "# number of occurrences of each word.\n",
    "word_counts = words.groupBy(\"word\").count()\n",
    "\n",
    "# Show results in the console\n",
    "word_counts.show()\n",
    "\n",
    "# Write the results to a Parquet file, a columnar storage file format optimized for reading speed.\n",
    "word_counts.write.parquet(\"word_counts.parquet\")\n",
    "\n",
    "# Write the results to a text file, note that this requires a single column DataFrame\n",
    "# For a simple text file output, you might want to convert it to RDD or use format-specific methods\n",
    "word_counts.rdd.map(lambda r: r[0] + \": \" + str(r[1])).saveAsTextFile(\"word_counts.txt\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4207d1ec",
   "metadata": {},
   "source": [
    "## Spark Examples: Sample data.csv below\n",
    "\n",
    "id,name,age,department,salary\n",
    "1,John Doe,58,Finance,55000\n",
    "2,Jane Smith,34,Marketing,62000\n",
    "3,Bob Johnson,45,Human Resources,45000\n",
    "4,Lisa Sway,30,IT,76000\n",
    "5,Michael Vicks,60,Finance,83000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d68b2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Basic Data Loading and Transformation\n",
    "# Loading a CSV File and Applying Transformations:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Loading Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load a CSV file into a DataFrame\n",
    "df = spark.read.csv(\"file:///path/to/your/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Select specific columns and add a new column\n",
    "transformed_df = df.select(\"name\", \"age\").withColumn(\"senior\", df.age > 55)\n",
    "\n",
    "# Show the transformed data\n",
    "transformed_df.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff702236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Using Spark SQL for Data Querying\n",
    "# Creating Temporary Views and Running SQL Queries:\n",
    "\n",
    "# Assuming 'spark' is already created and 'df' is loaded as above\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Execute SQL query\n",
    "result = spark.sql(\"\"\"\n",
    "SELECT name, age, senior\n",
    "FROM people\n",
    "WHERE senior = TRUE\n",
    "\"\"\")\n",
    "\n",
    "result.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and Writing a DataFrame to Parquet\n",
    "# Writing to Parquet: This format is highly efficient for both storage and processing. It’s a columnar storage \n",
    "# file format, which allows for better compression and enhanced query performance by allowing I/O to be performed\n",
    "# at the column level rather than at the row level.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Write and Read Parquet Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create some data\n",
    "data = [\n",
    "    (1, \"John Doe\", 58, \"Finance\", 55000),\n",
    "    (2, \"Jane Smith\", 34, \"Marketing\", 62000),\n",
    "    (3, \"Bob Johnson\", 45, \"Human Resources\", 45000),\n",
    "    (4, \"Lisa Sway\", 30, \"IT\", 76000),\n",
    "    (5, \"Michael Vicks\", 60, \"Finance\", 83000)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Write DataFrame to Parquet\n",
    "df.write.parquet(\"path/to/output/data.parquet\")\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f62d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Aggregations and Grouping\n",
    "# Grouping Data and Computing Aggregations:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Aggregations and Grouping\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from Parquet\n",
    "df = spark.read.parquet(\"path/to/output/data.parquet\")\n",
    "\n",
    "# Group by one column and calculate aggregates\n",
    "aggregated_df = df.groupBy(\"department\").agg(\n",
    "    count(\"id\").alias(\"num_employees\"),\n",
    "    avg(\"salary\").alias(\"average_salary\")\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "aggregated_df.show()\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2acdb41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4439c81",
   "metadata": {},
   "source": [
    "## Integrating with Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc15284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection parameters\n",
    "conn_params = {\n",
    "    \"dbname\": 'postgres',\n",
    "    \"user\": '',  # Replace 'your_username' with your actual PostgreSQL username\n",
    "    \"password\": '',  # Replace 'your_password' with your actual PostgreSQL password\n",
    "    \"host\": 'localhost'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8628c3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Writing Data to PostgreSQL\n",
    "## ensure that the public.persons table exists in your database or \n",
    "## modify the script to create the table if it does not exist.\n",
    "## download jdbc jar from https://jdbc.postgresql.org/download/\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with PostgreSQL JDBC driver included\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark PostgreSQL Integration\") \\\n",
    "    .config(\"spark.jars\", \"/Users/phinnx.com/Downloads/postgresql-42.7.3.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define connection parameters\n",
    "conn_params = {\n",
    "    \"user\": \"\",  # replace 'your_username' with your actual PostgreSQL username\n",
    "    \"password\": \"\",  # replace 'your_password' with your actual PostgreSQL password\n",
    "}\n",
    "\n",
    "# Sample DataFrame to write\n",
    "data = [(\"James\", \"Smith\", \"USA\", 1),\n",
    "        (\"Michael\", \"Rose\", \"USA\", 2),\n",
    "        (\"Robert\", \"Williams\", \"USA\", 3),\n",
    "        (\"Maria\", \"Jones\", \"USA\", 4)]\n",
    "columns = [\"firstname\", \"lastname\", \"country\", \"id\"]\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# JDBC URL\n",
    "url = \"jdbc:postgresql://localhost/postgres\"\n",
    "\n",
    "# Write DataFrame to PostgreSQL table\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", \"public.persons\") \\\n",
    "    .option(\"user\", conn_params[\"user\"]) \\\n",
    "    .option(\"password\", conn_params[\"password\"]) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "940ad3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from PostgreSQL table\n",
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", \"public.persons\") \\\n",
    "    .option(\"user\", conn_params[\"user\"]) \\\n",
    "    .option(\"password\", conn_params[\"password\"]) \\\n",
    "    .load()\n",
    "\n",
    "jdbcDF.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "880eae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4924c1d4",
   "metadata": {},
   "source": [
    "## Streaming Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97e69f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Python script that sends data to a socket. This can simulate streaming data\n",
    "\n",
    "import socket\n",
    "import time\n",
    "\n",
    "# Create a socket object\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# Use localhost\n",
    "host = 'localhost'  # or use '127.0.0.1'\n",
    "\n",
    "# Reserve a port for your service\n",
    "port = 9999\n",
    "\n",
    "# Bind to the port\n",
    "s.bind((host, port))\n",
    "\n",
    "# Wait for client connection\n",
    "s.listen(5)\n",
    "print(f\"Server listening on port {port}...\")\n",
    "\n",
    "# Establish connection with client\n",
    "conn, addr = s.accept()\n",
    "print(f\"Connected by {addr}\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Send data\n",
    "        conn.send(\"Hello Spark Streaming\\n\".encode('utf-8'))\n",
    "        time.sleep(1)  # slow down the loop to simulate streaming data\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped by the user\")\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dff3a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## PySpark script to read this data and perform real-time analytics\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Network Word Count\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read text from a socket (streaming)\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(expr(\"explode(split(value, ' ')) as word\"))\n",
    "\n",
    "# Count each word\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "# Output the results to the console\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46734ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
