{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90caae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully to broker\n",
      "Enter message to publish or type 'exit' to quit: ab\n",
      "Enter message to publish or type 'exit' to quit: cd\n"
     ]
    }
   ],
   "source": [
    "## -- Install paho-mqtt library using pip\n",
    "## pip install paho-mqtt==1.6.1\n",
    "\n",
    "\"\"\"Publish terminal messages to topic\"\"\"\n",
    "\n",
    "import paho.mqtt.client as mqtt\n",
    "\n",
    "# MQTT settings\n",
    "broker_url = \"3.138.185.79\"\n",
    "broker_port = 1883\n",
    "username = \"auca\"\n",
    "password = \"gishushu\"\n",
    "topic = \"auca_class\"\n",
    "client_id = \"my_mqtt_client\"\n",
    "\n",
    "# Callback when the client receives a CONNACK response from the server\n",
    "def on_connect(client, userdata, flags, rc):\n",
    "    if rc == 0:\n",
    "        print(\"Connected successfully to broker\")\n",
    "    else:\n",
    "        print(f\"Failed to connect, return code {rc}\\n\")\n",
    "        # If the client fails to connect then we should stop the loop\n",
    "        client.loop_stop()\n",
    "\n",
    "# Create a new instance of the MQTT client with a specific client ID\n",
    "client = mqtt.Client(client_id, clean_session=True)\n",
    "client.on_connect = on_connect  # attach the callback function to the client\n",
    "client.username_pw_set(username, password)  # set username and password\n",
    "\n",
    "client.connect(broker_url, broker_port, 60)  # connect to the broker\n",
    "\n",
    "# Start the network loop in a separate thread\n",
    "client.loop_start()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        message = input(\"Enter message to publish or type 'exit' to quit: \")\n",
    "        if message.lower() == 'exit':\n",
    "            break\n",
    "        client.publish(topic, message, qos=2)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Program interrupted by user, exiting...\")\n",
    "\n",
    "# Stop the network loop and disconnect\n",
    "client.loop_stop()\n",
    "client.disconnect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9b9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83839e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Subscribe to messages on topic\"\"\"\n",
    "\n",
    "\n",
    "import paho.mqtt.client as mqtt\n",
    "\n",
    "# MQTT settings\n",
    "broker_url = \"3.138.185.79\"\n",
    "broker_port = 1883\n",
    "username = \"auca\"\n",
    "password = \"gishushu\"\n",
    "topic = \"auca_class\"\n",
    "client_id = \"my_mqtt_client_subscriber\"\n",
    "\n",
    "# Callback when the client receives a CONNACK response from the server\n",
    "def on_connect(client, userdata, flags, rc):\n",
    "    if rc == 0:\n",
    "        print(\"Connected successfully to broker\")\n",
    "        # Subscribe to the topic once connected\n",
    "        client.subscribe(topic, qos=2)\n",
    "    else:\n",
    "        print(f\"Failed to connect, return code {rc}\\n\")\n",
    "\n",
    "# Callback for when a PUBLISH message is received from the server\n",
    "def on_message(client, userdata, msg):\n",
    "    print(f\"Message received on topic {msg.topic}: {msg.payload.decode()}\")\n",
    "\n",
    "# Create a new instance of the MQTT client with a specific client ID\n",
    "client = mqtt.Client(client_id, clean_session=True)\n",
    "client.on_connect = on_connect  # attach the connection callback function to the client\n",
    "client.on_message = on_message  # attach the message callback function to the client\n",
    "\n",
    "client.username_pw_set(username, password)  # set username and password\n",
    "client.connect(broker_url, broker_port, 60)  # connect to the broker\n",
    "\n",
    "# Start the network loop in a separate thread\n",
    "client.loop_forever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd5df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56b09d67",
   "metadata": {},
   "source": [
    "## Create a standard PostgreSQL table:\n",
    "\n",
    "CREATE TABLE sensors (\n",
    "   time        TIMESTAMPTZ       NOT NULL,\n",
    "   location    TEXT              NOT NULL,\n",
    "   device      TEXT              NOT NULL,\n",
    "   voltage     DOUBLE PRECISION  NOT NULL,\n",
    "   current     DOUBLE PRECISION  NOT NULL,\n",
    "   frequency   DOUBLE PRECISION  NOT NULL,\n",
    "   power       DOUBLE PRECISION  NOT NULL,\n",
    "   energy      DOUBLE PRECISION  NOT NULL,\n",
    "   PRIMARY KEY (time, location, device)\n",
    ");\n",
    "\n",
    "\n",
    "## Convert the table to a hypertable. Specify the name of the table you want to convert, and the column that holds its time values.\n",
    "\n",
    "SELECT create_hypertable('sensors', 'time', chunk_time_interval => interval '15 minutes');\n",
    "\n",
    "\n",
    "## Alter chunk_interval if need be: Impact on New Chunks Only\n",
    "SELECT set_chunk_time_interval('sensors', INTERVAL '20 minutes');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e69ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adf3aa28",
   "metadata": {},
   "source": [
    "\n",
    "## Insert Single Row\n",
    "\n",
    "INSERT INTO sensor (time, location, device, voltage, current, frequency, power, energy) \n",
    "VALUES \n",
    "('2024-04-26 12:30:00+00', 'Factory A', 'Sensor 123', 230.5, 5.5, 60, 1267.5, 15000);\n",
    "\n",
    "\n",
    "## Insert Multiple Rows\n",
    "\n",
    "INSERT INTO sensors (time, location, device, voltage, current, frequency, power, energy) \n",
    "VALUES \n",
    "('2024-04-26 12:31:00+00', 'Factory A', 'Sensor 123', 230.5, 5.5, 60, 1267.5, 15000),\n",
    "('2024-04-26 12:35:00+00', 'Factory B', 'Sensor 124', 220.0, 6.0, 50, 1320.0, 16000),\n",
    "('2024-04-26 12:40:00+00', 'Factory C', 'Sensor 125', 240.0, 4.8, 50, 1152.0, 14000);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d3c76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11125836",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fba0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable Compression\n",
    "\n",
    "ALTER TABLE sensor SET (timescaledb.compress, timescaledb.compress_orderby = 'time');\n",
    "\n",
    "## Enable Compression; adding location and device can be advantageous for data retrieval and compression efficiency as this approach can help TimescaleDB more effectively compress data by grouping similar values together, which is especially useful when your queries often filter or join on these columns.\n",
    "\n",
    "ALTER TABLE sensor SET (timescaledb.compress, timescaledb.compress_orderby = 'time', timescaledb.compress_segmentby = 'location, device');\n",
    "\n",
    "\n",
    "## Add Compression Policy\n",
    "SELECT add_compression_policy('sensor', INTERVAL '7 days');\n",
    "\n",
    "## Show all chunks older than x minutes\n",
    "SELECT show_chunks('sensor', older_than => INTERVAL '3 minutes');\n",
    "\n",
    "## Show which chunks are available and their compression status:\n",
    "SELECT chunk_name, range_start, range_end, is_compressed \n",
    "FROM timescaledb_information.chunks\n",
    "WHERE hypertable_name = 'sensor';\n",
    "\n",
    "\n",
    "## Compress all chunks older than 3 minutes\n",
    "SELECT compress_chunk(i)\n",
    "FROM show_chunks('sensor', older_than => INTERVAL '2 minutes') AS i;\n",
    "\n",
    "## Drop Table\n",
    "DROP TABLE TABLE_NAME;\n",
    "DROP TABLE sensor;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291f114",
   "metadata": {},
   "source": [
    "## Enable Compression\n",
    "ALTER TABLE sensor SET (timescaledb.compress, timescaledb.compress_orderby = 'time');\n",
    "\n",
    "## Enable Compression; adding location and device can be advantageous for data retrieval and compression efficiency as this approach can help TimescaleDB more effectively compress data by grouping similar values together, which is especially useful when your queries often filter or join on these columns.\n",
    "\n",
    "ALTER TABLE sensor SET (timescaledb.compress, timescaledb.compress_orderby = 'time', timescaledb.compress_segmentby = 'location, device');\n",
    "\n",
    "## Add Compression Policy\n",
    "SELECT add_compression_policy('sensor', INTERVAL '7 days');\n",
    "\n",
    "## Show all chunks older than x minutes\n",
    "SELECT show_chunks('sensors', older_than => INTERVAL '3 minutes');\n",
    "\n",
    "## Compress all chunks older than 3 minutesÂ¶\n",
    "SELECT compress_chunk(i) FROM show_chunks('sensor', older_than => INTERVAL '3 minutes') AS i;\n",
    "\n",
    "## Drop Table\n",
    "DROP TABLE TABLE_NAME; \n",
    "DROP TABLE sensor;\n",
    "\n",
    "## Disk Size of a hypertable; both compressed and uncompressed chunks\n",
    "SELECT hypertable_size('sensor');\n",
    "\n",
    "## Retrieves the name and size of each hypertable present in the database\n",
    "SELECT 'sensors', hypertable_size(format('%I.%I', hypertable_schema, hypertable_name)::regclass) FROM timescaledb_information.hypertables;\n",
    "\n",
    "## Detailed view of the disk space usage of a hypertable; If running on a distributed hypertable, ordering by node_name would show the size distribution across different data nodes \n",
    "SELECT * FROM hypertable_detailed_size('sensors') ORDER BY node_name;\n",
    "\n",
    "## Manually compresses a specific chunk identified by its internal name.\n",
    "SELECT compress_chunk( '_timescaledb_internal._hyper_1_1_chunk');\n",
    "\n",
    "## Provides compression statistics for all chunks of the specified hypertable.\n",
    "SELECT * FROM chunk_compression_stats('sensor');\n",
    "\n",
    "## Attempts to compress chunks of the 'sensor' hypertable that were created between three weeks ago and one week ago.\n",
    "SELECT compress_chunk(i) from show_chunks('sensor', now() - interval '1 week', now() - interval '3 weeks') i;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03310c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29752f6c",
   "metadata": {},
   "source": [
    "## Use the psycopg2 library to interact with your PostgreSQL (TimescaleDB) database\n",
    "\n",
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35871c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"No Batch Inserts\"\"\"\n",
    "\n",
    "import psycopg2\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import time  # Import the time module\n",
    "\n",
    "# Database connection parameters\n",
    "conn_params = {\n",
    "    \"dbname\": 'postgres',\n",
    "    \"user\": '',\n",
    "    \"password\": '',\n",
    "    \"host\": 'localhost'\n",
    "}\n",
    "\n",
    "# Function to generate random data, now with slightly adjusted time parameter\n",
    "def generate_random_data(sensor_id, fixed_time, delta_seconds):\n",
    "    time_adjusted = fixed_time + timedelta(seconds=delta_seconds)\n",
    "    return {\n",
    "        \"time\": time_adjusted,\n",
    "        \"location\": f\"Location {sensor_id}\",\n",
    "        \"device\": f\"Sensor {sensor_id}\",\n",
    "        \"voltage\": random.uniform(220, 240),\n",
    "        \"current\": random.uniform(10, 20),\n",
    "        \"frequency\": random.uniform(50, 60),\n",
    "        \"power\": random.uniform(1000, 5000),\n",
    "        \"energy\": random.uniform(500, 1500)\n",
    "    }\n",
    "\n",
    "conn = psycopg2.connect(**conn_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    print(\"Data Insertion Started...\")\n",
    "    start_time = time.time()  # Capture start time\n",
    "    fixed_time = datetime.now()\n",
    "    for i in range(200000):\n",
    "        sensor_id = random.randint(1, 10)\n",
    "        data = generate_random_data(sensor_id, fixed_time, i * 0.001)  # Adjust time by 0.001 second increments\n",
    "\n",
    "        query = \"\"\"\n",
    "        INSERT INTO sensor (time, location, device, voltage, current, frequency, power, energy) \n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s);\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (data['time'], data['location'], data['device'], data['voltage'],\n",
    "                               data['current'], data['frequency'], data['power'], data['energy']))\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            conn.commit()\n",
    "            ## print(f\"Inserted {i+1} records\")\n",
    "\n",
    "    conn.commit()\n",
    "    print(\"Data insertion complete.\")\n",
    "\n",
    "    end_time = time.time()  # Capture end time\n",
    "    total_time = end_time - start_time  # Calculate duration\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "    conn.rollback()\n",
    "\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c1338",
   "metadata": {},
   "source": [
    "## Use batch insertions for faster writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf05b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras  # Import extras for execute_batch\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Database connection parameters\n",
    "conn_params = {\n",
    "    \"dbname\": 'postgres',\n",
    "    \"user\": '',\n",
    "    \"password\": '',\n",
    "    \"host\": 'localhost'\n",
    "}\n",
    "\n",
    "# Function to generate random data, now with slightly adjusted time parameter\n",
    "def generate_random_data(sensor_id, fixed_time, delta_seconds):\n",
    "    time_adjusted = fixed_time + timedelta(seconds=delta_seconds)\n",
    "    return (\n",
    "        time_adjusted,\n",
    "        f\"Location {sensor_id}\",\n",
    "        f\"Sensor {sensor_id}\",\n",
    "        random.uniform(220, 240),\n",
    "        random.uniform(10, 20),\n",
    "        random.uniform(50, 60),\n",
    "        random.uniform(1000, 5000),\n",
    "        random.uniform(500, 1500)\n",
    "    )\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(**conn_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    print(\"Data Insertion Started...\")\n",
    "    start_time = time.time()  # Capture start time\n",
    "    fixed_time = datetime.now()\n",
    "\n",
    "    # Prepare data list for batch insert\n",
    "    data_tuples = []\n",
    "    for i in range(200000):\n",
    "        sensor_id = random.randint(1, 10)\n",
    "        data = generate_random_data(sensor_id, fixed_time, i * 0.001)  # Adjust time by 0.001 second increments\n",
    "        data_tuples.append(data)\n",
    "\n",
    "        # Execute batch every 1000 inserts\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            psycopg2.extras.execute_batch(cursor, \"\"\"\n",
    "            INSERT INTO sensor (time, location, device, voltage, current, frequency, power, energy)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s);\n",
    "            \"\"\", data_tuples)\n",
    "            conn.commit()\n",
    "            data_tuples = []  # Reset the list\n",
    "            ## print(f\"Inserted {i + 1} records\")\n",
    "\n",
    "    # Insert any remaining data\n",
    "    if data_tuples:\n",
    "        psycopg2.extras.execute_batch(cursor, \"\"\"\n",
    "        INSERT INTO sensor (time, location, device, voltage, current, frequency, power, energy)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s);\n",
    "        \"\"\", data_tuples)\n",
    "        conn.commit()\n",
    "\n",
    "    print(\"Data insertion complete.\")\n",
    "\n",
    "    end_time = time.time()  # Capture end time\n",
    "    total_time = end_time - start_time  # Calculate duration\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "    conn.rollback()\n",
    "\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328bd7ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "934ec7a2",
   "metadata": {},
   "source": [
    "\n",
    "## SQL Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Average, Max, and Min voltage readings for each device\n",
    "SELECT device, AVG(voltage) AS avg_voltage, MAX(voltage) AS max_voltage, MIN(voltage) AS min_voltage\n",
    "FROM sensor\n",
    "GROUP BY device;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2dbf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TimescaleDB provides a function called time_bucket() which is used to aggregate data over a \n",
    "## period of time. This is particularly useful for creating downsampled data views.\n",
    "## -- Average current readings in 10-minute intervals for each device\n",
    "SELECT time_bucket('10 minutes', time) AS period, device, AVG(current) AS avg_current\n",
    "FROM sensor\n",
    "GROUP BY period, device\n",
    "ORDER BY period, device;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac10da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## indexes that can optimize time-based lookups are not effective or absent\n",
    "CREATE INDEX idx_sensor_device_time ON sensor(device, time);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50e876",
   "metadata": {},
   "source": [
    "\n",
    "## Python-SQL Analytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2216b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Establish connection\n",
    "conn = psycopg2.connect(dbname='postgres', user='username', password='password', host='localhost')\n",
    "\n",
    "# Query average daily energy consumption per device\n",
    "query = \"\"\"\n",
    "SELECT time_bucket('1 day', time) AS day, device, AVG(energy) AS avg_daily_energy\n",
    "FROM sensor\n",
    "GROUP BY day, device\n",
    "ORDER BY day, device;\n",
    "\"\"\"\n",
    "\n",
    "# Load query results into a DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d416ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute basic statistics across all sensors to understand variations in energy use.\n",
    "\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check how many data points are available\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b873e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Visualization: Plot energy readings across sensors to visualize differences.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df['device'], df['avg_daily_energy'])\n",
    "plt.xlabel('Device')\n",
    "plt.ylabel('Average Daily Energy')\n",
    "plt.title('Energy Consumption by Sensor')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5138da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clustering: Group sensors based on their energy consumption characteristics using \n",
    "## clustering algorithms. KMeans is a popular clustering algorithm used for partitioning data \n",
    "## into K distinct, non-overlapping clusters.\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming data normalization is appropriate here\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "df['cluster'] = kmeans.fit_predict(df[['avg_daily_energy']])\n",
    "print(df[['device', 'avg_daily_energy', 'cluster']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a9d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple Comparisons and Correlations\n",
    "## You can perform comparisons or check for correlations between sensors based on the available metrics.\n",
    "\n",
    "# Example: Comparing average energy consumption\n",
    "max_energy_device = df.loc[df['avg_daily_energy'].idxmax(), 'device']\n",
    "print(f\"Device with maximum average energy consumption: {max_energy_device}\")\n",
    "\n",
    "min_energy_device = df.loc[df['avg_daily_energy'].idxmin(), 'device']\n",
    "print(f\"Device with minimum average energy consumption: {min_energy_device}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
